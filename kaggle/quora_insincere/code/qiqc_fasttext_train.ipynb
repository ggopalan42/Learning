{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from fastText import train_supervised\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paths\n",
    "training_data = Path('//data/data1/datasets/kaggle/quora_insincere/qiqc_train_set.txt')\n",
    "model_path = Path('/data/data1/datasets/kaggle/quora_insincere/models/qiqc_fasttext_def.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print results of the test\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext_default(train_data, epochs=25, learning_rate=1.0, wordNgrams=2):\n",
    "    # train_supervised uses the same arguments and defaults as the fastText cli\n",
    "    model = train_supervised(\n",
    "        input=train_data, epoch=epochs, lr=learning_rate, wordNgrams=wordNgrams, verbose=2, minCount=1\n",
    "    )\n",
    "    # print_results(*model.test(valid_data))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext_losshs(train_data, epochs=25, learning_rate=1.0, wordNgrams=2):\n",
    "    model = train_supervised(\n",
    "        input=train_data, epoch=epochs, lr=learning_rate, wordNgrams=wordNgrams, verbose=2, minCount=1,\n",
    "        loss=\"hs\"\n",
    "    )\n",
    "    # print_results(*model.test(valid_data))\n",
    "    # model.save_model(\"cooking.bin\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_quantize(train_data, retrain=True, cutoff=100000):\n",
    "    model.quantize(input=train_data, qnorm=True, retrain=retrain, cutoff=cutoff)\n",
    "    # print_results(*model.test(valid_data))\n",
    "    # model.save_model(\"cooking.ftz\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run default train\n",
    "model_default = train_fasttext_default(str(training_data.absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.save_model(str(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__0',), array([1.00001001]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_default.predict('my voice range is a2-c5 .  my chest voice goes up to f4 .  included sample in my higher chest range .  what is my voice typ')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
